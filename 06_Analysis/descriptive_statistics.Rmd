---
title: "Descriptive Statistics"
author: "Erin Buchanan"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Libraries

```{r libraries, message = F}
library(rio)
library(dplyr)
library(ggplot2)
library(tidyverse)
```

# Merge Data

```{r}
native.trans <- import("../05_Data_real/native_trans.xlsx")
browser.lang <- import("../05_Data_real/browser_language.xlsx")
```

# Participant level data

We will present descriptive statistics on the participants involved in the study including percentages of gender, education levels, native language, and average age. Information about the device used to complete the study will include percentages of computer operating system, the web browser, and the language locale (i.e., the language the browser defaults to using). Finally, the sample sizes collected by the collaborating labs will be provided. Each of these statistics will be provided for the overall data and the data separated by language.

```{r import-participant}
# open all participant data
p.list <- list.files(
  path = "../05_Data_real/data_processing/output_data",
  full.names = TRUE,
  recursive = TRUE,
  pattern = "*.participant_data.csv"
)

# import files
p.files <- lapply(p.list, import)

# give them names 
names(p.files) <- gsub("../05_Data_real/data_processing/output_data/participant_data/|_participant_data.csv", "", p.list)

# deal with mismatches
for (i in 1:length(p.files)){
  p.files[[i]]$url_lab <- as.character(p.files[[i]]$url_lab)
}

# merge them together
p.df <- bind_rows(p.files)
p.df$language <- rep(names(p.files), times = unlist(lapply(p.files, nrow)))

# native translation
p.df <- p.df %>% 
  mutate(native_language = tolower(native_language)) %>% 
  left_join(
    native.trans %>% 
      select(listed, `Official Translation`) %>% 
      dplyr::rename(native_language_translated = `Official Translation`) %>% 
      mutate(listed = tolower(listed)) %>% 
      unique(),
    by = c("native_language" = "listed")
  ) %>% 
  mutate(native_language_translated = 
           ifelse(is.na(native_language)|native_language == "", "Missing", 
                  native_language_translated)) 

# recode missing
p.df <- p.df %>% 
   mutate(please_tell_us_your_gender = 
           ifelse(is.na(please_tell_us_your_gender)|please_tell_us_your_gender == "", "Missing", 
                  please_tell_us_your_gender), 
          please_tell_us_your_education_level = 
           ifelse(is.na(please_tell_us_your_education_level)|please_tell_us_your_education_level == "", "Missing", 
                  please_tell_us_your_education_level),
          meta_platform = ifelse(is.na(meta_platform)|meta_platform == "",
          "Missing", meta_platform))

# age
p.df <- p.df %>% 
  mutate(year_exp = as.numeric(substr(timestamp_consent,1,4)), 
         age = year_exp - which_year_were_you_born)

# browser
browsers <- ua_parse(p.df$meta_user_agent) %>% 
  unique() %>% 
  select(userAgent, ua.family)

p.df <- p.df %>% 
  left_join(
    browsers, 
    by = c("meta_user_agent" = "userAgent")
  ) %>% 
  mutate(meta_locale = tolower(meta_locale)) %>% 
  left_join(
    browser.lang %>% 
      dplyr::rename("browser_language" = "Language") %>% 
      unique(),
    by = c("meta_locale" = "Code")
  )

# ones to fix
temp <- 
  as.data.frame(table(p.df$meta_locale, 
                      p.df$browser_language, useNA = "ifany")) %>% 
  filter(Freq > 0) %>% 
  filter(is.na(Var2))

temp
```

```{r overall-percents}
# overall version
p.overall.gender <- p.df %>% 
  group_by(please_tell_us_your_gender, keep) %>% 
  summarize(freq.gender = n() / nrow(.) * 100,
            .groups = "keep") 
p.overall.education <- p.df %>% 
  group_by(please_tell_us_your_education_level, keep) %>% 
  summarize(freq.edu = n() / nrow(.) * 100,
            .groups = "keep") 
p.overall.native <- p.df %>% 
  group_by(native_language_translated, keep) %>% 
  summarize(freq.lang = n() / nrow(.) * 100,
            .groups = "keep")  
p.overall.age <- p.df %>% 
  group_by(keep) %>% 
  summarize(m.age = mean(age, na.rm = T),
            sd.age = sd(age, na.rm = T),
            .groups = "keep") 
p.overall.os <- p.df %>% 
  group_by(meta_platform, keep) %>% 
  summarize(freq.os = n() / nrow(.) * 100,
            .groups = "keep") 
p.overall.web <- p.df %>% 
  group_by(ua.family, keep) %>% 
  summarize(freq.browser = n() / nrow(.) * 100,
            .groups = "keep") 
p.overall.lang.locale <- p.df %>% 
  group_by(browser_language, keep) %>% 
  summarize(freq.locale = n() / nrow(.) * 100,
            .groups = "keep") 

p.overall.gender
p.overall.education
p.overall.native
p.overall.age
p.overall.os
p.overall.web
p.overall.lang.locale

# match no match native language (in the paper)
# combine all the multiples (in the supplement)
# all split by language in excel docs with a description of where they are at 
```

```{r language-percents}
# language version
p.summary.demos <- p.df %>% 
  group_by(language) %>% 
  summarize(n = n()) %>% 
  # gender 
  left_join (
    p.df %>% 
      group_by(language, please_tell_us_your_gender) %>% 
      summarize(freq.gender = n(), 
                .groups = "keep") %>% 
      pivot_wider(names_from = please_tell_us_your_gender, 
                  values_from = freq.gender,
                  id_cols = language),
    by = "language"
    ) %>% 
  dplyr::rename("missing_gender" = "Missing") %>% 
  # education
  left_join (
  p.df %>% 
    group_by(language, please_tell_us_your_education_level) %>% 
    summarize(freq.edu = n(), 
              .groups = "keep") %>% 
    pivot_wider(names_from = please_tell_us_your_education_level, 
                values_from = freq.edu,
                id_cols = language),
  by = "language"
  ) %>% 
dplyr::rename("missing_lang" = "Missing") %>% 
  # age
  left_join(
    p.df %>% 
      group_by(language) %>% 
      summarize(m.age = mean(age, na.rm = T),
            sd.age = sd(age, na.rm = T), 
            .groups = "keep"),
    by = "language"
  )

export(p.summary.demos, "", row.names = F)

p.summary.native <- 
    p.df %>% 
      group_by(language, native_language_translated) %>% 
      summarize(freq.lang = n(), 
                .groups = "keep")

p.summary.computer <- 
    p.df %>% 
      group_by(language, meta_platform) %>% 
      summarize(freq.os = n(), 
                .groups = "keep") %>% 
      pivot_wider(names_from = meta_platform, 
                  values_from = freq.os,
                  id_cols = language) %>% 
  dplyr::rename("missing_os" = "Missing") %>% 
  left_join (
    p.df %>% 
      group_by(language, ua.family) %>% 
      summarize(freq.browser = n(), 
                .groups = "keep") %>% 
      pivot_wider(names_from = ua.family, 
                  values_from = freq.browser,
                  id_cols = language),
    by = "language"
    ) 

p.summary.locale <- 
    p.df %>% 
      group_by(language, browser_language) %>% 
      summarize(freq.locale = n(), 
                .groups = "keep") 

# look up meta language, meta locale 
```

```{r sample-lab}
# merge this with lab table from online 
p.sample.lab <- p.df %>% 
  group_by(url_lab, language) %>% 
  summarize(n.lab = n(), 
            .groups = "keep")
```

make a map picture by where labs were recruiting 
- figure out how to deal with prolific, mturk, qualtrics, bilendi 

# Trial level data

Each language will be saved in a separate file with an item specific trial identification number to allow for matching concepts across languages (i.e., CAT [English] → KATZE [German] → GATTA [Italian]). If a participant leaves the study early (e.g., Internet disconnection, computer crash, closes the study), the data past this point in the study is not recorded, and therefore, the trial level data represents all trials displayed during the experiment. Participants are expected to incorrectly answer trials, and these trials will be marked for exclusion. All timeout trials will be marked as missing values in the final data. No missing values will be imputed.

We will mark for exclusion minimum response latencies of less than 160 ms (i.e., all trials will be presented in the trial level data for openness, but these will be excluded for analysis and calculations listed below). The response latencies from each participant’s session will then be z-scored in line with recommendations from Faust et al.. We will not collect enough data to note if a person takes the experiment multiple times for privacy reasons, but as these would be considered different sessions, the recommended z-score procedure should control for participant variability at this level. Therefore, repeated participation would not be detrimental to data collection. Finally, participants' overall proportion of correct answers will be calculated, and participants who do not correctly answer at least 80% of 100 minimum trials will be excluded for item data, priming data, and analysis. The average error in the Semantic Priming Project ranged from 4% to 5%, and this criterion was chosen to include participants who were focused on the task.

We will provide descriptive statistics on the average time to complete the study, the number of trials by word type (word, nonword), the accuracy by word type, and average z-scored response latencies by word type (overall, excluding Z > 2.5, excluding Z > 3.0; see below). These values will be provided for overall results and separated by language.

```{r}

```

# Item level data

The item file will contain lexical information about all stimuli calculated from the OpenSubtitles23 and subs2vec55 projects (length, frequency, orthographic neighborhood, bigram frequency, orthographic and phonographic Levenshtein distance). The descriptive statistics calculated from the trial level data will then be included: mean response latency, average standardized response latency, sample size, standard errors of response latencies, and accuracy rate. No data will be excluded for being a potential outlier; however, we will recommend a cut-off criterion for absolute value z-score outliers at 2.5 and 3.0, and we will calculate these same statistics with those subsets of trials excluded. For all real words, the age of acquisition, imageability, concreteness, valence, dominance, arousal, and familiarity values will be included. These values do not exist for non-words.
24

We will provide descriptive statistics on the average sample size, average z-scored response latencies, and average SE for the z-scored response latencies by each word type (word, nonword). These values will be calculated for the overall data set, separated by language, and without each level of z-score outlier criterion.

# Priming data

add note about overall proportions of priming trials 
calculate transition probabilities 
ridgeline plot 

In a separate file, we will also prepare information about priming results which includes the target word, average response latencies, averaged Z-scored response latencies, sample sizes, standard errors, and priming response latency. For each item, priming is defined as the average z-scored response latency when presented in the unrelated minus the related condition. Therefore, the timing for DOG-CAT would be subtracted from BUS-CAT to indicate priming for the word CAT. The similarity scores calculated during stimuli selection will be provided in this file, as well as other popular measures of similarity if they are available in that language. For example, semantic feature overlap norms are also available in Italian95, German96, Spanish26, and Dutch97.
We will provide the average statistics for z-score priming, z-score unrelated response latency, z-score related response latency, sample size for unrelated trials, and sample size for related trials. These values will be calculated overall, by language, and with/without z- score level exclusions. Last, we will calculate the participant level priming reliability98 and item-level priming reliability44.
Exclusion summary
Data will be excluded for the following reasons in this order:
1) Participant level data: the entire participant’s data will be removed from the analyses.
a) Participant did not indicate at least 18 years of age.
b) Participant did not complete at least 100 trials.
c) Participant did not achieve 80% correct.
2) Trial level data: only the individual trials will be removed from the analyses.
a) Timeout trials (i.e., no response given in 3 s window).
25

b) Incorrectly answered trials.
c) Response latencies shorter than 160 ms.
3) Trial level exclusions dependent on test: trials marked for exclusion that are tested
with and without these values in the hypotheses described below.
a) Response latencies over the absolute value of Z = 2.5.
b) Response latencies over the absolute value of Z = 3.0.
Hypothesis 1
Hypothesis information is presented in Table 1. Hypothesis 1 predicts semantic facilitation with reduced response latencies for related than unrelated words. Hypothesis 1 will be analyzed by calculating an intercept-only regression model using the z-scored priming response latency as the dependent variable. The intercept and its 95% confidence interval will represent the grand mean of the priming effect across all languages. The priming response latency is calculated by taking the average of the unrelated pair z-scored response latency minus the related pair response latency within each item. Therefore, values that are positive and greater than zero (e.g., > 0.0001) indicate priming because the related pair had a faster response latency than the unrelated pair. We will determine support for Hypothesis 1 if the lower limit of the confidence interval is greater than zero (i.e., a directional comparison). This process will be repeated for average priming scores calculated without trials that were marked as 2.50 z-score outliers and 3.00 z-score outliers separately. The decision criteria will remain the same, and we will identify any differences in decisions based on outlier statistics (e.g., priming only occurs when X trials are removed).
Hypothesis 2
Hypothesis 2 explores the extent to which these semantic priming effects vary across languages. Therefore, we will calculate a random effects model using the nlme99 package in R wherein the random intercept of language will be added to the overall intercept only model for Hypothesis 1. We will report the standard deviation of the random effect, its 95% confidence interval, the AIC change between models, and the pseudo-R2 values for the
26

effect size of this parameter100. Results will support significant heterogeneity when the AIC for the random effects model is two points or more less than the AIC for the intercept-only model56. This analysis will be repeated with the 2.50 z-score outliers and 3.00 z-score outliers excluded. We will include a forest plot of the priming effect and their 95% confidence intervals to visualize the potential heterogeneity in the priming results. Simulations of models within and without variability in the priming effects can be found at https://osf.io/fbhr8/.