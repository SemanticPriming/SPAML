---
title: "SPAML Stimuli Creation"
author: "Erin M. Buchanan"
date: "Last Update `r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Libraries

```{r messages = F, warning = F}
library(rio, quietly = T)
library(tm, quietly = T)
library(udpipe, quietly = T)
library(plyr, quietly = T)
library(R.utils, quietly = T)
library(NCmisc, quietly = T)
library(dplyr, quietly = T)
library(tm, quietly = T)
library(lsa, quietly = T)
library(quanteda, quietly = T)
options(timeout=2000)
```

# User Defined Set Up

```{r}
root <- paste0(getwd(), "/")
dir.create(paste0(root, 'similarity/'), showWarnings = F)
word_choice <- c("NOUN", "VERB", "ADJ", "ADV")
en_special_list <- c("don", "didn", "wouldn", "couldn", "isn", "wasn", "shouldn", "haven", "ain", "gonna", "gotta", "doesn", "wanna")
```

# Function Set Up

## Importing Data Function

This section imports the subtitles data from the Open Subtitles website and breaks down the files into smaller components for processing. The second function imports information from the `subs2vec` project to leverage preprocessed data. 

```{r}
import_subs <- function (language, root){
  
  dir.create(paste0(root, 'data/', language), showWarnings = F)
  dir.create(paste0(root, 'frequency/', language), showWarnings = F)
  
  con <-
  paste0("http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/mono/OpenSubtitles.",
         language, ".gz")
  
  
  # goes into root / data / language
  download.file(
  con,
  destfile = paste0(root, "data/", language, '/', language, ".gz"),
  mode = "wb")
  
  text_file <-
  gunzip(
    filename = paste0(root, "data/", language, '/', language, ".gz"),
    destname = paste0(root, "data/", language, '/', language, ".txt"))
  
  file.split(
  text_file,
  size = 100000,
  same.dir = TRUE,
  verbose = TRUE,
  suf = "part",
  #win = .Platform$OS.type == 'windows'
  win = TRUE)
  
  rm(con, text_file)
  
}

import_other <- function(language, what) {
  
  dir.create(paste0(root, 'data/', language), showWarnings = F)
  dir.create(paste0(root, 'frequency/', language), showWarnings = F)
  
  con <-
  paste0(overall_languages[overall_languages$language_code == language , what ])
  
  
  # goes into root / data / language
  download.file(
  con,
  destfile = paste0(root, "data/", language, '/', language, ".zip"),
  mode = "wb")

}
```

## Calculate Frequency

This section calculates the word frequency given the subtitles by looping over each small subtitle file to create the part of speech and token/lemma. 

```{r}
# input model language, which file to process 
calc_frequency <- function(root, model_language, file_name) {
  
  data.text <-
    readLines(
      file_name,
      encoding = "UTF-8")
  
  # Preprocess text
  data.text <- tolower(data.text)
  data.text <- tm::stripWhitespace(data.text)
  
  # Annotate the Text
  annotate <- udpipe(data.text, model_language, parser = "none") 
  annotated_df <- as.data.frame(annotate)
  
  # Subset columns
  annotated_df <- annotated_df[ , c("token", "lemma", "upos", "xpos")]
  
  # Frequency of unique combinations
  count_df <- plyr::count(annotated_df, colnames(annotated_df))
  
  file_name <- gsub("data/", "frequency/", file_name, fixed = T)
  file_name <- paste0(file_name, "_frequency.csv")
  
  export(count_df, file = file_name)
  
  cat(file_name)
}
```

## Run the Files

This section examines all the files in the subtitle data and combines their results together.

```{r}
calc_all_files <- function (root, language, model_language) {
  
  file_names <-
  list.files(
    path = paste0(root, "data/", language),
    pattern = paste0(language, "_part"),
    full.names = T)

  for (current_file in file_names) {
    calc_frequency(root, model_language, current_file)
  }
  
}
```

## Create Summarized Frequency Results

```{r}

```

## Create Similarity from subs2vec

```{r}
calc_sim <- function(words, vec){
  # reduce the data
  vec <- subset(vec, V1 %in% words)
  rownames(vec) <- vec$V1
  vec <- vec[ , -1]
  vec <- t(vec)
  similarity <- cosine(as.matrix(vec))
  return(similarity)
}

# credit to Brenton Wiernik
top_n <- function(similarity, n) {
  n_cols <- ncol(similarity)
  final_sims <- data.frame(
    cue = vector("character", n * n_cols),
    target = vector("character", n * n_cols),
    cosine = vector("numeric", n * n_cols)
  )
  columns <- colnames(similarity)
  rows <- rownames(similarity)
  indices <- apply(similarity, 2, function(x) order(x, decreasing = TRUE)[seq_len(n)])
  for (i in seq_len(n_cols)) {
    final_sims[(i - 1) * n + 1:n,] <- list(
      columns[i],
      rows[indices[,i]],
      similarity[indices[,i],i]
    )
  }
  return(final_sims)
}
```

## Pseudoword Generator 

```{r}
# make up the fake words 
get_fake <- function(wordlist){

  bigrams <- as.data.frame(
    table(char_ngrams(
      unlist(tokens(wordlist, "character")), 
      concatenator = "")))
  
  new_words <- rep(NA, length(wordlist))

  for (i in 1:length(wordlist)){
  
  word <- wordlist[i]
  # pick a random letter
  num_replace <- sample(1:nchar(word), 1)
  char_replace <- substr(word, num_replace, num_replace)

    # if the first letter
    if (num_replace == 1){
      
      examine_replace <- substr(word, num_replace + 1, num_replace + 1)
      poss_replace <- sample(substr(
        bigrams$Var1[grepl(paste0(examine_replace, "$"), bigrams$Var1)], 1, 1),1)
      
      broken_down <- unlist(strsplit(word, split = ""))
      broken_down[num_replace] <- poss_replace
      new_words[i] <- paste(broken_down, collapse = "")
      
      # if the last letter
    } else if (num_replace == nchar(word)){
      
      examine_replace <- substr(word, num_replace - 1, num_replace - 1)
      poss_replace <- sample(substr(
        bigrams$Var1[grepl(paste0("^", examine_replace), bigrams$Var1)], 2, 2),1)

      broken_down <- unlist(strsplit(word, split = ""))
      broken_down[num_replace] <- poss_replace
      new_words[i] <- paste(broken_down, collapse = "")
      
      # if anything else 
    } else {
      
      examine_replace_before <- substr(word, num_replace - 1, num_replace - 1)
      examine_replace_after <- substr(word, num_replace + 1, num_replace + 1)
      
      intersection <- intersect(substr(bigrams$Var1[
        grepl(paste0("^", examine_replace_before), 
              bigrams$Var1)], 2, 2),
        substr(bigrams$Var1[
          grepl(paste0(examine_replace_after, "$"), 
                bigrams$Var1)], 1, 1))
      
      if (length(intersection) > 0 ){
        poss_replace <- sample(intersection, 1)
      } else { poss_replace <- sample(substr(bigrams$Var1[
        grepl(paste0("^", examine_replace_before), 
              bigrams$Var1)], 2, 2), 1)
        }
      
      broken_down <- unlist(strsplit(word, split = ""))
      broken_down[num_replace] <- poss_replace
      new_words[i] <- paste(broken_down, collapse = "")
    
    }
  }
  return(new_words) 
}
```

# Languages

## Import the Overall File

```{r}
overall_languages <- import("stimuli_options.xlsx")
# total open subs
nrow(overall_languages)

overall_languages <- subset(overall_languages, udpipe_model != "NA")
# total we can do
nrow(overall_languages)
```

- All languages from OpenSubtitles have been filtered for only languages with models in `udpipe` for part of speech tagging. 
- Is the language available on subs2vec? 
  - Yes, use `import_other` to import the `subs_count` file.
    - Does the language have 50,000 words?
      - Yes, tag the language with `udpipe` and use the `subs_vec` file to calculate similarity.
      - No, use `import_other` to import the `wiki_count` file.
        - Tag both the `subs_count` and `wiki_count` and merge together.
        - Use the `wiki_vec` file to calculate similarity. 
  - No, use `import_subs` to import the OpenSubtitles.
    - Use `udpipe` to tag the files and calculate frequency from that information. 
    - Use the `subs2strudel` or `words2many` project to calculate similarity.  
    
- Other exclusions:
  - Words must be three or more characters long
  - Words must be listed in `word_choice`
  - Stopwords will be excluded, if they are available for a language 

## af Afrikaans

```{r eval = F}
language <- "af"

# download other on subs count
import_other(language = "af", what = "subs_count")

# import subs to check size
af_subs <- import(paste0(root, "data/", language, "/", language, ".zip"))

# check size
nrow(af_subs)

# less than 50k so use wiki for frequency
import_other(language = "af", what = "wiki_count")

af_wiki <- import(paste0(root, "data/", language, "/", language, ".zip"))

# check size
nrow(af_wiki)

# tag with udpipe
af_tagged <- udpipe(af_wiki$unigram, object = overall_languages$udpipe_model[overall_languages$language_code == "af"], parser = "none")

# exclusions
af_tagged <- subset(af_tagged, nchar(af_tagged$lemma) >= 3)
af_tagged <- subset(af_tagged, upos %in% word_choice)
af_tagged <- subset(af_tagged, !(lemma %in% tm::stopwords(kind = "SMART")))
af_tagged <- subset(af_tagged, !(lemma %in% af_special_list))
af_tagged <- subset(af_tagged, !(grepl("[0-9]", af_tagged$sentence)))

# merge frequency back into tagged list
# merge by sentence so one to one match
colnames(af_subs) <- c("sentence", "freq")
af_final <- merge(af_tagged, af_subs, by = "sentence", all.x = T)

# eliminate duplicates by lemma
af_final <- af_final[order(af_final$freq, decreasing = TRUE) , ]
af_final <- af_final[!duplicated(af_final$lemma), ]

# grab top 10K
af_top <- af_final[1:10000 , ]

# find similarity
import_other(language = "af", what = "subs_vec")

# load similarity vec
utils::unzip(zipfile = paste0(root, "data/", language, '/', language, ".zip"),
             exdir = paste0(root, "data/", language))
file.rename(paste0(root, "data/", language, "/subs.", language, ".1e6.vec"),
            paste0(root, "data/", language, "/subs.", language, ".1e6.txt"))
af_vec <- read.table(paste0(root, "data/", language, "/subs.", language, ".1e6.txt"), 
                     quote="\"")

# calculate similarity, take top 6, drop cosine of cue-target match
af_sim <- calc_sim(af_top$lemma, af_vec)
af_top_sim <- top_n(af_sim, 6)
af_top_sim <- subset(af_top_sim, cue!=target)

# create fake words
af_top_sim$fake_cue <- get_fake(af_top_sim$cue)
af_top_sim$fake_target <- get_fake(af_top_sim$target)

write.csv(af_top_sim, paste0(root, "similarity/", language, "_sims.csv"))
```


## en English

```{r eval = F}
language <- "en"

# download other on subs count
import_other(language = "en", what = "subs_count")

# import subs to check size
en_subs <- import(paste0(root, "data/", language, "/", language, ".zip"))

# check size
nrow(en_subs)

# tag with udpipe
en_tagged <- udpipe(en_subs$unigram, object = overall_languages$udpipe_model[overall_languages$language_code == "en"], parser = "none")

# exclusions
en_tagged <- subset(en_tagged, nchar(en_tagged$lemma) >= 3)
en_tagged <- subset(en_tagged, upos %in% word_choice)
en_tagged <- subset(en_tagged, !(lemma %in% tm::stopwords(kind = "SMART")))
en_tagged <- subset(en_tagged, !(lemma %in% en_special_list))
en_tagged <- subset(en_tagged, !(grepl("[0-9]", en_tagged$sentence)))

# merge frequency back into tagged list
# merge by sentence so one to one match
colnames(en_subs) <- c("sentence", "freq")
en_final <- merge(en_tagged, en_subs, by = "sentence", all.x = T)

# eliminate duplicates by lemma
en_final <- en_final[order(en_final$freq, decreasing = TRUE) , ]
en_final <- en_final[!duplicated(en_final$lemma), ]

# grab top 10K
en_top <- en_final[1:10000 , ]

# find similarity
import_other(language = "en", what = "subs_vec")

# load similarity vec
utils::unzip(zipfile = paste0(root, "data/", language, '/', language, ".zip"),
             exdir = paste0(root, "data/", language))
file.rename(paste0(root, "data/", language, "/subs.", language, ".1e6.vec"),
            paste0(root, "data/", language, "/subs.", language, ".1e6.txt"))
en_vec <- read.table(paste0(root, "data/", language, "/subs.", language, ".1e6.txt"), 
                     quote="\"")

# calculate similarity, take top 6, drop cosine of cue-target match
en_sim <- calc_sim(en_top$lemma, en_vec)
en_top_sim <- top_n(en_sim, 6)
en_top_sim <- subset(en_top_sim, cue!=target)

# create fake words
en_top_sim$fake_cue <- get_fake(en_top_sim$cue)
en_top_sim$fake_target <- get_fake(en_top_sim$target)

write.csv(en_top_sim, paste0(root, "similarity/", language, "_sims.csv"))
```



## zh_tw Mandarin 

```{r}
import_subs(language = "zh_tw", 
            root = root)
calc_all_files(root = root, 
               language = "zh_tw",
               model_language = overall_languages$udpipe_model[overall_languages$language_code == "zh_tw"])
```


## Afrikaans

## 




# Examine the Results

## Afrikaans

In this section, we will see if the results are comparable from the subs2vec project to leverage already processed data. 

```{r}
# our processed data
af_freq <- import("frequency/af/af_partaa.txt_frequency.csv")
af_summary <- af_freq %>% group_by(token) %>% summarize(calc_freq = sum(freq))
pop_pos <- af_freq[order(af_freq$freq, decreasing = T) , ]
pop_pos <- pop_pos[!duplicated(pop_pos$token) , ]
af_summary <- merge(af_summary, pop_pos[ , c("token", "upos") ], by = "token")


# subs2vec data
af_subs <- import("https://hdl.handle.net/1839/a1a82d13-be9d-4024-bbfe-90de100fcd3c@download")
colnames(af_subs) <- c("token", "freq")
af_subs_summary <- udpipe(af_subs$token, model_language, parser = "none")
af_subs_summary <- af_subs_summary[ , c("token", "upos")]
af_subs_summary <- merge(af_subs_summary, af_subs, by = "token")

af_together <- merge(af_summary, af_subs_summary, by = "token", all = T)
```

How many overall matches do we get?

```{r}
nrow(af_together)
nrow(na.omit(af_together))

af_all <- na.omit(af_together)

# take out less than two letters
af_all <- subset(af_all, nchar(af_all$token) > 2)
nrow(af_all)
```

How correlated are the top most frequent words?

```{r}
cor(af_all$calc_freq, af_all$freq)
plot(af_all$calc_freq, af_all$freq)

MSE <- af_all$calc_freq - af_all$freq

mean(MSE)
sd(MSE)
hist(MSE)
```

How much do the parts of speech match?

```{r}
words_want <- c("NOUN", "VERB", "ADJ", "ADV", "PRON", "PROPN", "INTJ", "NUM")
sum(af_all$upos.x == af_all$upos.y) / nrow(af_all)

af_just_want <- subset(af_all, upos.x %in% words_want | upos.y %in% words_want)
sum(af_just_want$upos.x == af_just_want$upos.y) / nrow(af_all)

table("estimated" = af_all$upos.y, "calculated" = af_all$upos.x)
```

Would we pick the same approximate words?

```{r}
top_calculated <- subset(af_all, upos.x %in% words_want)
top_calculated <- top_calculated$token[order(top_calculated$calc_freq, decreasing = T)][1:5000]

top_estimated <- subset(af_all, upos.y %in% words_want)
top_estimated <- top_estimated$token[order(top_estimated$freq, decreasing = T)][1:5000]

sum(top_estimated %in% top_calculated) / 5000
```




## Japanese

```{r}
# our processed data
ja_freq <- import_list("frequency/ja/ja_archive.zip", rbind = T)
ja_summary <- ja_freq %>% group_by(token) %>% summarize(calc_freq = sum(freq))
pop_pos <- ja_freq[order(ja_freq$freq, decreasing = T) , ]
pop_pos <- pop_pos[!duplicated(pop_pos$token) , ]
ja_summary <- merge(ja_summary, pop_pos[ , c("token", "upos") ], by = "token")


# subs2vec data
ja_subs <- import("https://hdl.handle.net/1839/a1a82d13-be9d-4024-bbfe-90de100fcd3c@download")
colnames(ja_subs) <- c("token", "freq")
ja_subs_summary <- udpipe(ja_subs$token, model_language, parser = "none")
ja_subs_summary <- ja_subs_summary[ , c("token", "upos")]
ja_subs_summary <- merge(ja_subs_summary, ja_subs, by = "token")

ja_together <- merge(ja_summary, ja_subs_summary, by = "token", all = T)
```

How many overall matches do we get?

```{r}
nrow(ja_together)
nrow(na.omit(ja_together))

ja_all <- na.omit(ja_together)

# take out less than two letters
ja_all <- subset(ja_all, nchar(ja_all$token) > 2)
nrow(ja_all)
```

How correlated are the top most frequent words?

```{r}
cor(ja_all$calc_freq, ja_all$freq)
plot(ja_all$calc_freq, ja_all$freq)

MSE <- ja_all$calc_freq - ja_all$freq

mean(MSE)
sd(MSE)
hist(MSE)
```

How much do the parts of speech match?

```{r}
words_want <- c("NOUN", "VERB", "ADJ", "ADV", "PRON", "PROPN", "INTJ", "NUM")
sum(ja_all$upos.x == ja_all$upos.y) / nrow(ja_all)

ja_just_want <- subset(ja_all, upos.x %in% words_want | upos.y %in% words_want)
sum(ja_just_want$upos.x == ja_just_want$upos.y) / nrow(ja_all)

table("estimated" = ja_all$upos.y, "calculated" = ja_all$upos.x)
```

Would we pick the same approximate words?

```{r}
top_calculated <- subset(ja_all, upos.x %in% words_want)
top_calculated <- top_calculated$token[order(top_calculated$calc_freq, decreasing = T)][1:5000]

top_estimated <- subset(ja_all, upos.y %in% words_want)
top_estimated <- top_estimated$token[order(top_estimated$freq, decreasing = T)][1:5000]

sum(top_estimated %in% top_calculated) / 5000
```
