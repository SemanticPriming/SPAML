---
title: 'What can we learn about two million priming values? An update to the Semantic Priming Across Many Languages Project (PSA007)'
author: "Erin M. Buchanan & The Psychological Science Accelerator"
institute: "Harrisburg University"
format: 
  revealjs:
    theme: night
editor: source
incremental: true 
scrollable: true 
preview-links: true
code-copy: true 
highlight-style: github 
editor_options: 
  chunk_output_type: inline
---

## Many Thanks{.smaller}

```{css}
h1.title {
font-size: 1.5em;
}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(dplyr)
library(rio)
library(tidyr)
library(ggridges)
library(nlme)
library(broom)
library(ggrepel)
```

- The Psychological Science Accelerator
- ZPID
- Harrisburg University
- Over 100 labs and 300+ collaborators
- Many translators
- The admin team 

## Semantic Priming{.smaller} 

- Semantic priming - a tale as old as time
- Usually measured with the lexical decision or naming task 
- Example:
  - SPOON-DOG (unrelated, slower than)
  - CAT-DOG (related)
- The Semantic Priming Project (Hutchison et al., 2013) provided priming values for 1661 *English* word-pairs

## Semantic Priming{.smaller}

- Rich history of research
- Issues:
  - WEIRD words 
  - Single language focus or multilingual individuals 
  - A lack of data sets that are matched on language within one study 

## Study Goals{.smaller}

- How can we use the skill sets proposed in natural language processing research and the new large open dataset? 
- Goals of of the SPAML:
  - Assess semantic priming across (at least) 10 languages using matched stimuli for the registered report - more languages are included!
  - Provide a large-scale data set for reuse in linguistics 
- Registered Report at *Nature Human Behaviour*

## The Stimuli{.smaller}

- Corpus Text Data: [Open Subtitles Project](http://opus.nlpl.eu/OpenSubtitles-v2018.php)
- Subtitles are incredibly useful for understanding natural language and word frequency (New et al., 2007; Brysbaert & New, 2009; Keuleers et al., 2010; Cuetos et al., 2012; Van Heuven et al., 2014; Mandera et al., 2015; and more)
- Freely available subtitles in 63 languages for computational analysis
- Approximately 43 languages contain enough of the right data to be usable for these projects
  - And we added one or two based on interest 

## The Stimuli{.smaller}

- For each language:
  - Collect the top 10,000 most frequent nouns, verbs, adjectives, and adverbs
  - Find the top five most similar words using cosine from subs2vec (van Paridon & Thompson, 2021)
  - Cross-reference this list across languages 
  - Pick the most overlapping stimuli limiting repeats and proper names
  - 1000 final pairs
- Important: driven by the language, *not* English translation
  
## Nonwords and Translators{.smaller}

- Nonwords are generated with a Wuggy-like algorithm (Keuleers & Brysbaert, 2010)
- Translators check all pairs for proper translation, form, and meaning
- They suggest the appropriate words for retaining meaning between cue-target
- They fix nonwords to ensure they are pronounceable, not too fake
- Dialects are considered and separated when appropriate 

## Procedure{.smaller} 

- View a simple version: https://psa007.psysciacc.org/
- Overall task: 
  - A single stream lexical decision task 
  - All words cue-target are judged, cue-target linked by order 
- Trials are formatted as:
  - A fixation cross (+) for 500 ms
  - CUE or TARGET in Serif font
  - Lexical decision response (word, nonsense word)
  - 400 pairs = 800 trials
- Keyboards are WILD

## Power and Study Design{.smaller} 

- Power focused on using bootstrapping and accuracy in parameter estimation (see Ken Kelley's work)
- We simulated using the English Lexicon Project and Semantic Priming Project
  - Minimum: *n* = 50 per target word by condition (related, unrelated)
  - Stopping: *SE* = .09
  - Maximum = *n* = 320 
- Adaptive sampling checks and samples pairs once an hour to randomize the study 

## Data Provided{.smaller}

- The data will be provided in several forms: 
  - Participant level: participant information
  - Trial level: all individual trials
  - Item level: for each individual item, rather than just cue or just concept
  - Priming level: for each related pair compared to the unrelated pair
  - We'll talk about each of these
  
## Participant Level Data{.smaller}

```{r}
files <- list.files(path = "processed_data",
                    pattern = "*_part.csv",
                    full.names = TRUE)

list.import <- lapply(files, import)

PDF <- bind_rows(list.import) 
```

- 119 labs have received a data collection link
- 25431 total participants 
- 21408 total "usable participants"
  - At least 18
  - At least 100 trials
  - At least 80% accuracy 
- Unfortunately, this is lower data retention than we anticipated 
  - SPP was approximately 95%
  - Ours is 84%
  
## Participant Level Data{.smaller}

```{r out.height="100%", out.width="100%"}
PDF <- PDF %>% na.omit()

# total number of participants 27 languages 
ggplot(
  PDF %>% 
  group_by(language) %>% 
  summarize(total = sum(total)), 
  aes(x = language, 
      y = total)) + 
    geom_bar(stat = "identity") + 
    theme_linedraw() + 
    xlab("Study Language") + 
    ylab("Total Participants") + 
  geom_hline(yintercept = 800)
```

## Participant Level Data{.smaller}

- For this talk, we are going to split the data into:
  - Match: Took the study in the language of their browser
  - No Match: Took the study in a different language than their browser
- A proxy for multilingualism
  - Native language is included but not totally processed yet

## Participant Level Data{.smaller}

```{r out.height="100%", out.width="100%"}
# number of participants by language match by usable or not by language
ggplot(PDF %>% 
         filter(lang_match == "FALSE"), 
       aes(x = language, y = total, fill = keep_participant)) + 
  geom_bar(position = "fill",
            stat = "identity") + 
  theme_linedraw() + 
  xlab("Study Language") + 
  ylab("Percent Usable") + 
  scale_fill_discrete(name = "Keep Participant", 
                      labels = c("No", "Yes")) + 
  theme(legend.position = "bottom") + 
  ggtitle("Language: No Match")
```

## Participant Level Data{.smaller}

```{r out.height="100%", out.width="100%"}
ggplot(PDF %>% 
         filter(lang_match == "TRUE"), 
       aes(x = language, y = total, fill = keep_participant)) + 
  geom_bar(position = "fill",
            stat = "identity") + 
    theme_linedraw() + 
  xlab("Study Language") + 
  ylab("Percent Usable") + 
  scale_fill_discrete(name = "Keep Participant", 
                      labels = c("No", "Yes")) + 
  theme(legend.position = "bottom") + 
  ggtitle("Language: Match")
```

## Trial Level Data{.smaller}

```{r out.height="100%", out.width="100%"}
files <- list.files(path = "processed_data",
                    pattern = "*_trial.csv",
                    full.names = TRUE)

list.import <- lapply(files, import)
TDF <- bind_rows(list.import) %>% 
  na.omit()
```

- Total number of trials: `r sum(TDF$total)`
- Total number of usable trials `r TDF %>% filter(keep == "keep") %>% pull(total) %>% sum()`

## Trial Level Data{.smaller}

```{r out.height="100%", out.width="100%"}
# number of participants by language match by usable or not by language
ggplot(TDF %>% 
         filter(lang_match == "FALSE"), 
       aes(x = language, y = total, fill = keep)) + 
  geom_bar(position = "fill",
            stat = "identity") + 
  theme_linedraw() + 
  xlab("Study Language") + 
  ylab("Percent Usable") + 
  scale_fill_discrete(name = "Keep Trial", 
                      labels = c("No", "Yes")) + 
  theme(legend.position = "bottom") + 
  ggtitle("Language: No Match")
```

## Trial Level Data{.smaller}

```{r out.height="100%", out.width="100%"}
ggplot(TDF %>% 
         filter(lang_match == "TRUE"), 
       aes(x = language, y = total, fill = keep)) + 
  geom_bar(position = "fill",
            stat = "identity") + 
    theme_linedraw() + 
  xlab("Study Language") + 
  ylab("Percent Usable") + 
  scale_fill_discrete(name = "Keep Trial", 
                      labels = c("No", "Yes")) + 
  theme(legend.position = "bottom") + 
  ggtitle("Language: Match")
```

## Item Level Data{.smaller}

```{r out.height="100%", out.width="100%"}
files <- list.files(path = "processed_data",
                    pattern = "*_item.csv",
                    full.names = TRUE)

list.import <- lapply(files, import) 
IDF <- bind_rows(list.import) %>% 
  mutate(language = TDF$language)

ggplot(IDF %>% 
         filter(lang_match == "FALSE"), 
       aes(x = language, y = accuracy, fill = class)) + 
  geom_bar(stat = "identity",
           position = "dodge") + 
  theme_linedraw() + 
  xlab("Study Language") + 
  ylab("Percent Accuracy") + 
  scale_fill_discrete(name = "Trial Type", 
                      labels = c("Nonword", "Word")) + 
  theme(legend.position = "bottom") + 
  coord_cartesian(ylim = c(.75,1)) + 
  ggtitle("Language: No Match")
```

## Item Level Data{.smaller}

```{r out.height="100%", out.width="100%"}
ggplot(IDF %>% 
         filter(lang_match == "TRUE"), 
       aes(x = language, y = accuracy, fill = class)) + 
  geom_bar(stat = "identity",
           position = "dodge") + 
  theme_linedraw() + 
  xlab("Study Language") + 
  ylab("Percent Accuracy") + 
  scale_fill_discrete(name = "Trial Type", 
                      labels = c("Nonword", "Word")) + 
  theme(legend.position = "bottom") + 
  coord_cartesian(ylim = c(.75,1)) + 
  ggtitle("Language: Match")
```

## Item Level Data{.smaller}

```{r out.height="100%", out.width="100%"}
ggplot(IDF %>% 
         filter(lang_match == "FALSE"), 
       aes(x = language, y = MZRT, fill = class)) + 
  geom_bar(stat = "identity",
           position = "dodge") + 
  theme_linedraw() + 
  xlab("Study Language") + 
  ylab("Mean Z-Score RT") + 
  scale_fill_discrete(name = "Trial Type", 
                      labels = c("Nonword", "Word")) + 
  theme(legend.position = "bottom")  + 
  ggtitle("Language: No Match")
```

## Item Level Data{.smaller}

```{r out.height="100%", out.width="100%"}
ggplot(IDF %>% 
         filter(lang_match == "TRUE"), 
       aes(x = language, y = MZRT, fill = class)) + 
  geom_bar(stat = "identity",
           position = "dodge") + 
  theme_linedraw() + 
  xlab("Study Language") + 
  ylab("Mean Z-Score RT") + 
  scale_fill_discrete(name = "Trial Type", 
                      labels = c("Nonword", "Word")) + 
  theme(legend.position = "bottom")  + 
  ggtitle("Language: Match")
```

## Priming Level Data{.smaller}

- Priming data is calculated by:
  - {SPOON}-DOG trial for DOG *minus* {CAT}-DOG trial for DOG
  - Matched on the target
  - *Across* participants 
- That means usually you need complete data to calculate interesting prime information 
- Here we are using data with at least 20 people per target (not our real minimum)
  - Also calculating priming as unrelated condition mean *minus* related condition mean 

## Priming Level Data{.smaller}

```{r out.height="100%", out.width="100%"}
files <- list.files(path = "processed_data",
                    pattern = "*_priming.csv",
                    full.names = TRUE)

list.import <- lapply(files, import)
num_rows <- unlist(lapply(list.import, nrow))
languages <- unique(TDF$language)

primeDF <- bind_rows(list.import) %>% 
  mutate(language = rep(languages, times = num_rows)) %>% 
  select(language, 
         target_word_unique,
         avgZ_prime_FALSE, 
         avgZ_prime_TRUE,
         avgZ_RT_related_FALSE, 
         avgZ_RT_related_TRUE,
         avgZ_RT_unrelated_FALSE, 
         avgZ_RT_unrelated_TRUE,
         # target_correct_keep_related_FALSE, 
         # target_correct_keep_related_TRUE,
         # target_correct_keep_unrelated_FALSE,
         # target_correct_keep_unrelated_TRUE,
         target_sample_keep_related_FALSE,
         target_sample_keep_related_TRUE,
         target_sample_keep_unrelated_FALSE,
         target_sample_keep_unrelated_TRUE
         )

# nrow(primeDF)

all.20 <- primeDF %>% 
  filter(target_sample_keep_related_FALSE >= 20 & 
         target_sample_keep_related_TRUE >= 20 & 
         target_sample_keep_unrelated_FALSE >= 20 & 
         target_sample_keep_unrelated_TRUE)

# need to pivot
all.20.long <- all.20 %>% 
  select(language, avgZ_prime_FALSE, avgZ_prime_TRUE, target_word_unique) %>% 
  pivot_longer(
    cols = c(avgZ_prime_FALSE, avgZ_prime_TRUE), 
    names_to = "lang_match",
    values_to = "primingRT"
  )

primeDF.long <- bind_rows( 
  primeDF %>% 
  select(language, 
         avgZ_RT_related_FALSE, 
         target_sample_keep_related_FALSE,
         target_word_unique) %>% 
  rename(size = target_sample_keep_related_FALSE, 
         z_RT = avgZ_RT_related_FALSE) %>% 
    mutate(type = "related",
           lang_match = FALSE),
  primeDF %>% 
  select(language, 
         avgZ_RT_unrelated_FALSE, 
         target_sample_keep_unrelated_FALSE,
         target_word_unique) %>% 
  rename(size = target_sample_keep_unrelated_FALSE, 
         z_RT = avgZ_RT_unrelated_FALSE) %>% 
    mutate(type = "unrelated",
           lang_match = FALSE), 
  primeDF %>% 
  select(language, 
         avgZ_RT_related_TRUE, 
         target_sample_keep_related_TRUE,
         target_word_unique) %>% 
  rename(size = target_sample_keep_related_TRUE, 
         z_RT = avgZ_RT_related_TRUE) %>% 
    mutate(type = "related",
           lang_match = TRUE),
  primeDF %>% 
  select(language, 
         avgZ_RT_unrelated_TRUE, 
         target_sample_keep_unrelated_TRUE,
         target_word_unique) %>% 
  rename(size = target_sample_keep_unrelated_TRUE, 
         z_RT = avgZ_RT_unrelated_TRUE) %>% 
    mutate(type = "unrelated",
           lang_match = TRUE)
  ) %>% 
  filter(size >= 20)

# pivot and then do anything with at least 20 
ggplot(all.20.long, 
       aes(y = language, x = primingRT, fill = lang_match)) + 
  geom_density_ridges(alpha = .4,
                      scale = .8) + 
  theme_linedraw() + 
  xlab("Z RT Priming") + 
  ylab("Language") + 
  scale_fill_discrete(name = "Language", 
                      labels = c("No Match", "Match")) + 
  theme(legend.position = "bottom")
```

## Priming Level Data{.smaller}

```{r out.height="100%", out.width="100%"}
ggplot(primeDF.long %>% 
         filter(lang_match == "FALSE"), 
       aes(y = language, x = z_RT, fill = type)) + 
  geom_density_ridges(alpha = .4,
                      scale = .8) + 
  theme_linedraw() + 
  xlab("Z RT") + 
  ylab("Language") + 
  scale_fill_discrete(name = "Trial", 
                      labels = c("Related", "Unrelated")) + 
  theme(legend.position = "bottom") + 
  coord_cartesian(xlim = c(-1.1, 1.1)) + 
  ggtitle("Language: No Match")
```

## Priming Level Data{.smaller}

```{r out.height="100%", out.width="100%"}
ggplot(primeDF.long %>% 
         filter(lang_match == "TRUE"), 
       aes(y = language, x = z_RT, fill = type)) + 
  geom_density_ridges(alpha = .4,
                      scale = .8) + 
  theme_linedraw() + 
  xlab("Z RT") + 
  ylab("Language") + 
  scale_fill_discrete(name = "Trial", 
                      labels = c("Related", "Unrelated")) + 
  theme(legend.position = "bottom") + 
  coord_cartesian(xlim = c(-1.1, 1.1)) + 
  ggtitle("Language: Match")
```

## Priming Level Data{.smaller}

- All match: 
  - Priming ~ .09
  - No differences in language match 
- Just unrelated - related separately by condition: 
  - Priming ~ .11
  - Interaction Language Match with Priming
    - No Match ~ .09
    - Match ~ .11

```{r}
model.prime.all.diff <- lme(
  primingRT ~ lang_match,
  random = list(~1|language),
  data = all.20.long
)

# summary(model.prime.all.diff)
# coef(model.prime.all.diff)

model.prime.diff <- lme(
  z_RT ~ type,
  random = list(~1|language),
  data = primeDF.long
)

# summary(model.prime.diff)

model.prime.diff.match <- lme(
  z_RT ~ type*lang_match,
  random = list(~1|language),
  data = primeDF.long
)

# summary(model.prime.diff.match)

model.prime.diff.matchT <- lme(
  z_RT ~ type,
  random = list(~1|language),
  data = primeDF.long %>% 
    filter(lang_match == "TRUE")
)

# summary(model.prime.diff.matchT)

model.prime.diff.matchF <- lme(
  z_RT ~ type,
  random = list(~1|language),
  data = primeDF.long %>% 
    filter(lang_match == "FALSE")
)

# summary(model.prime.diff.matchF)
```

## Priming Level Data{.smaller}

```{r out.height="100%", out.width="100%"}
getwd()
matched <- read.csv("../../03_Materials/matched_stimuli/matched_stimuli.csv")

primeDF.cross <- bind_rows( 
  primeDF %>% 
  select(language, 
         avgZ_RT_related_FALSE, 
         target_sample_keep_related_FALSE,
         target_word_unique) %>% 
  rename(size = target_sample_keep_related_FALSE, 
         z_RT = avgZ_RT_related_FALSE) %>% 
    mutate(type = "related",
           lang_match = FALSE),
  primeDF %>% 
  select(language, 
         avgZ_RT_unrelated_FALSE, 
         target_sample_keep_unrelated_FALSE,
         target_word_unique) %>% 
  rename(size = target_sample_keep_unrelated_FALSE, 
         z_RT = avgZ_RT_unrelated_FALSE) %>% 
    mutate(type = "unrelated",
           lang_match = FALSE), 
  primeDF %>% 
  select(language, 
         avgZ_RT_related_TRUE, 
         target_sample_keep_related_TRUE,
         target_word_unique) %>% 
  rename(size = target_sample_keep_related_TRUE, 
         z_RT = avgZ_RT_related_TRUE) %>% 
    mutate(type = "related",
           lang_match = TRUE),
  primeDF %>% 
  select(language, 
         avgZ_RT_unrelated_TRUE, 
         target_sample_keep_unrelated_TRUE,
         target_word_unique) %>% 
  rename(size = target_sample_keep_unrelated_TRUE, 
         z_RT = avgZ_RT_unrelated_TRUE) %>% 
    mutate(type = "unrelated",
           lang_match = TRUE)
  ) %>% 
  # for weighted means
  mutate(z_RT = ifelse(is.na(z_RT), 0, z_RT),
         size = ifelse(is.na(size), 0, size)) %>% 
  group_by(type, target_word_unique, language) %>% 
  summarize(z_RT = stats::weighted.mean(z_RT, size, na.rm = T),
            size = sum(size),
            .groups = "keep") %>% 
  filter(size >= 20) %>% 
  pivot_wider(id_cols = c(target_word_unique, language),
              names_from = type, 
              values_from = z_RT) %>% 
  na.omit() %>% 
  mutate(primeZRT = unrelated - related) %>% 
  ungroup()

matched2 <- matched %>% 
  left_join(primeDF.cross %>% filter(language == "ar") %>% 
              select(target_word_unique, primeZRT),
            by = c("ar_target_word_unique" = "target_word_unique")) %>% 
  left_join(primeDF.cross %>% filter(language == "br_pt") %>% 
              select(target_word_unique, primeZRT),
            by = c("pt_br_target_word_unique" = "target_word_unique")) %>%
  left_join(primeDF.cross %>% filter(language == "cs") %>% 
              select(target_word_unique, primeZRT),
            by = c("cs_target_word_unique" = "target_word_unique")) %>%
  left_join(primeDF.cross %>% filter(language == "da") %>% 
              select(target_word_unique, primeZRT),
            by = c("da_target_word_unique" = "target_word_unique")) %>%
  left_join(primeDF.cross %>% filter(language == "de") %>% 
              select(target_word_unique, primeZRT),
            by = c("de_target_word_unique" = "target_word_unique")) %>%
  left_join(primeDF.cross %>% filter(language == "el") %>% 
              select(target_word_unique, primeZRT),
            by = c("el_target_word_unique" = "target_word_unique")) %>%
  left_join(primeDF.cross %>% filter(language == "en") %>% 
              select(target_word_unique, primeZRT),
            by = c("en_target_word_unique" = "target_word_unique")) %>%
  left_join(primeDF.cross %>% filter(language == "es") %>% 
              select(target_word_unique, primeZRT),
            by = c("es_target_word_unique" = "target_word_unique")) %>%
  left_join(primeDF.cross %>% filter(language == "fa") %>% 
              select(target_word_unique, primeZRT),
            by = c("fa_target_word_unique" = "target_word_unique")) %>%
  left_join(primeDF.cross %>% filter(language == "fr") %>% 
              select(target_word_unique, primeZRT),
            by = c("fr_target_word_unique" = "target_word_unique")) %>%
  left_join(primeDF.cross %>% filter(language == "he") %>% 
              select(target_word_unique, primeZRT),
            by = c("he_target_word_unique" = "target_word_unique")) %>%
  left_join(primeDF.cross %>% filter(language == "hu") %>% 
              select(target_word_unique, primeZRT),
            by = c("hu_target_word_unique" = "target_word_unique")) %>%
  left_join(primeDF.cross %>% filter(language == "it") %>% 
              select(target_word_unique, primeZRT),
            by = c("it_target_word_unique" = "target_word_unique")) %>%
  left_join(primeDF.cross %>% filter(language == "ja") %>% 
              select(target_word_unique, primeZRT),
            by = c("ja_target_word_unique" = "target_word_unique")) %>%
  left_join(primeDF.cross %>% filter(language == "ko") %>% 
              select(target_word_unique, primeZRT),
            by = c("ko_target_word_unique" = "target_word_unique")) %>%
  left_join(primeDF.cross %>% filter(language == "pl") %>% 
              select(target_word_unique, primeZRT),
            by = c("pl_target_word_unique" = "target_word_unique")) %>%
  left_join(primeDF.cross %>% filter(language == "pt") %>% 
              select(target_word_unique, primeZRT),
            by = c("pt_target_word_unique" = "target_word_unique")) %>%
  left_join(primeDF.cross %>% filter(language == "ro") %>% 
              select(target_word_unique, primeZRT),
            by = c("ro_target_word_unique" = "target_word_unique")) %>%
  left_join(primeDF.cross %>% filter(language == "ru") %>% 
              select(target_word_unique, primeZRT),
            by = c("ru_target_word_unique" = "target_word_unique")) %>%
  left_join(primeDF.cross %>% filter(language == "sk") %>% 
              select(target_word_unique, primeZRT),
            by = c("sk_target_word_unique" = "target_word_unique")) %>%
  left_join(primeDF.cross %>% filter(language == "tr") %>% 
              select(target_word_unique, primeZRT),
            by = c("tr_target_word_unique" = "target_word_unique")) %>%
  left_join(primeDF.cross %>% filter(language == "ur") %>% 
              select(target_word_unique, primeZRT),
            by = c("ur_target_word_unique" = "target_word_unique")) %>%
  left_join(primeDF.cross %>% filter(language == "zh") %>% 
              select(target_word_unique, primeZRT),
            by = c("zh_target_word_unique" = "target_word_unique")) %>% 
  select(starts_with("primeZRT"), en_cue, en_target)

matched2$prop_work <- apply(matched2 %>% select(-en_cue, -en_target), 1, function(x) sum(x > 0, na.rm = T) / length(na.omit(x)))

ggplot(matched2, aes(x = prop_work)) + 
  theme_linedraw() + 
  geom_histogram(bins = 25, fill = "white", color = "black") + 
  ylab("Frequency") + 
  xlab("Proportion that 'Work'")
```

## Priming Level Data{.smaller}

```{r}
matched2 %>% 
  filter(prop_work == 1) %>% 
  select(en_cue, en_target)
```

## Final Thoughts{.smaller} 

- We have a lot of data! It is so exciting!
- Participant retention is lower in the no match condition
- Of the usable participants: trial level retention is about the same 
- Overall, nonwords are harder to get right, especially in the no match condition 
- Nonword trials are slower than real word trials (WHEW)
- Interesting distributions for priming with completely matched data
- Very similar distributions when you look at the overall conditions
- A slightly *slightly* smaller effect for priming when participants take the study in a different language than their browser. 


## Questions?{.smaller}

:::: {.columns}
::: {.column width="70%"}
- Thank you for listening!
- All PSA collaborators are listed with their author information online
:::
::: {.column width="30%"}
```{r echo=FALSE, out.width="80%"}
knitr::include_graphics("QR_links.png")
```
:::
::::